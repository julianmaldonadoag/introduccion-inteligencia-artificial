{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4edc90a2",
   "metadata": {},
   "source": [
    "Estudiante: Maldonado Aguilar Angel Julian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35119996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7bf5cd",
   "metadata": {},
   "source": [
    "## Problema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67f78e",
   "metadata": {},
   "source": [
    "**Nota 1**: para poder visualizar la imagen, favor colocar la imagen del proyecto en el mismo directorio donde se encuentra esta libreta.\n",
    "\n",
    "**Nota 2**: A continuación se muestra la forma manual de resolver el problema, al final se encuentra la manera automatizada.\n",
    "\n",
    "![MDP proyecto](./mdp_ejercicio.png)\n",
    "\n",
    "En la imagen del problema se pueden identificar los siguientes elementos:\n",
    "\n",
    "1. **Estados**: { $S_1$, $S_2$, $S_3$, $S_4$, $S_5$, $S_6$ }\n",
    "2. **Acciones**: { $a_1$, $a_2$ }\n",
    "3. **Matriz de transición**: la cual define la probabilidad de transitar entre estados al aplicar diferentes acciones.\n",
    "4. **Recompensas**: las recompensas obtenidas en cada posible transición, las cuales para este problema puede ser una de las siguientes dos:\n",
    "    - 0\n",
    "    - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2349edac",
   "metadata": {},
   "source": [
    "# Proceso de decisión de Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c9b24",
   "metadata": {},
   "source": [
    "Un Procesos de decisión de Markov (MDP, Markov decision process) modela un problema de decisión sequencial en donde el sistema evoluciona en el tiempo y es controlado por un agente. La dinámica del sistema está determinada por una función de transición de probabilidad que mapea estados y acciones\n",
    "a otros estados.\n",
    "\n",
    "Formalmente un Proceso de Decisión de Markov es una tupla:\n",
    "\n",
    "$$MDP = (S, s_1, A, P, R)$$\n",
    "\n",
    "siendo \n",
    "- $S$ un conjunto de estados\n",
    "- $s_1$ el estado inicial\n",
    "- $A$ el conjunto de acciones\n",
    "- $P$ la matriz de transición (conjunto de matrices)\n",
    "- $R$ las rescompensas definidas en cada transicion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b8ba4c",
   "metadata": {},
   "source": [
    "## Matriz de transición"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b036b",
   "metadata": {},
   "source": [
    "Una vez definida la **política** se puede determinar la matriz de transición.\n",
    "\n",
    "Por ejemplo para el problema a resolver, se puede aplicar la política de que en cada estado, el agente debe realizar la acción $a_1$, por lo tanto la matriz de transición correspondiente es:\n",
    "\n",
    "$$\n",
    "P_{\\pi_1} = \n",
    "\\begin{bmatrix}\n",
    "0 & 0.3 & 0.7 & 0 & 0 & 0\\\\ \n",
    "0 & 0.3 & 0.7 & 0 & 0 & 0\\\\ \n",
    "0.4 & 0 & 0 & 0.6 & 0 & 0\\\\ \n",
    "0 & 0 & 0 & 0 & 0.6 & 0.4\\\\ \n",
    "0 & 0 & 0 & 0 & 0 & 0\\\\ \n",
    "0 & 0 & 0 & 0 & 0 & 0\\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Mientras que si la política es realizar la acción $a_2$, la matriz de transición es:\n",
    "\n",
    "$$\n",
    "P_{\\pi_2} = \n",
    "\\begin{bmatrix}\n",
    "0.3 & 0 & 0 & 0.7 & 0 & 0\\\\ \n",
    "0.3 & 0 & 0 & 0.7 & 0 & 0\\\\ \n",
    "0 & 0.4 & 0.6 & 0 & 0 & 0\\\\ \n",
    "0 & 0 & 0 & 0 & 0 & 0\\\\ \n",
    "0 & 0 & 0 & 0 & 0 & 0\\\\ \n",
    "0 & 0 & 0 & 0 & 0 & 0\\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f263d5",
   "metadata": {},
   "source": [
    "## Recompensa esperada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54670a8",
   "metadata": {},
   "source": [
    "Cuando un agente se encuentra en un estado, $s_t$, y aplica la acción $a_t$, recibirá una recompensa que dependerá del estado al que transite. Dado que puede transitar a varios estados, nos interesa evaluar el valor esperado de la recompensa:\n",
    "\n",
    "$$ R_{s_t, a_t} = E[R_{t+1} | s_t, a_t]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a62cb1",
   "metadata": {},
   "source": [
    "Para una política cualquiera, $\\pi$, podemos guardar los valores de la recompensa esperada para cada estado en una matriz (vector columna). Por ejemplo, para la politica $\\pi_1$ la recompensa sería:\n",
    "\n",
    "$$\n",
    "r_{\\pi_1} = \n",
    "\\begin{bmatrix}\n",
    "0.3\\times(1) + 0.7\\times(1) = 1 \\\\ \n",
    "0.3\\times(1) + 0.7\\times(1) = 1 \\\\ \n",
    "0.4\\times(1) + 0.6\\times(1) = 1 \\\\ \n",
    "0.6\\times(0) + 0.4\\times(0) = 0 \\\\ \n",
    "0 \\\\ \n",
    "0 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Mientras que la recompensa esperada para la política $\\pi_2$ es:\n",
    "\n",
    "$$\n",
    "r_{\\pi_2} = \n",
    "\\begin{bmatrix}\n",
    "0.3\\times(1) + 0.7\\times(1) = 1 \\\\ \n",
    "0.3\\times(1) + 0.7\\times(1) = 1 \\\\ \n",
    "0.4\\times(1) + 0.6\\times(1) = 1 \\\\ \n",
    "0 \\\\ \n",
    "0 \\\\ \n",
    "0 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7387a3d",
   "metadata": {},
   "source": [
    "## Politica óptima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e8ad35",
   "metadata": {},
   "source": [
    "El objetivo del aprendizaje por reforzamiento es el de identificar una política que maximice el valor esperado de las recompensas a largo plazo:\n",
    "\n",
    "$$ \n",
    "v_\\pi(s) = E\n",
    "\\begin{bmatrix}\n",
    "    \\sum_{i=1}^{\\infty} \\gamma^{t-1}r_t | s_1=s\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a0ecd7",
   "metadata": {},
   "source": [
    "### Ecuación de Bellman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d4dda",
   "metadata": {},
   "source": [
    "Ahora bien para determinar el valor de una política podemos utilizar la ecuación de Bellman, que en notación matricial es la siguiente:\n",
    "\n",
    "$$ v_\\pi = r_\\pi + \\gamma P_\\pi v_\\pi $$\n",
    "\n",
    "En la cual se puede despejar el valor de la política y nos quedaria:\n",
    "\n",
    "$$ v_\\pi = (I - \\gamma P_\\pi)^{-1} r_\\pi $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0046c2f6",
   "metadata": {},
   "source": [
    "**Valor de la política $\\pi_1$**\n",
    "\n",
    "Continuando con el problema, podemos evaluar el valor de la politica $\\pi_1$, la que realiza la acción $a_1$ a cada estado del problema.\n",
    "\n",
    "Recordemos que la matriz de transición es:\n",
    "\n",
    "$$\n",
    "P_{\\pi_1} = \n",
    "\\begin{bmatrix}\n",
    "0 & 0.3 & 0.7 & 0 & 0 & 0\\\\ \n",
    "0 & 0.3 & 0.7 & 0 & 0 & 0\\\\ \n",
    "0.4 & 0 & 0 & 0.6 & 0 & 0\\\\ \n",
    "0 & 0 & 0 & 0 & 0.6 & 0.4\\\\ \n",
    "0 & 0 & 0 & 0 & 0 & 0\\\\ \n",
    "0 & 0 & 0 & 0 & 0 & 0\\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Y el vector de recompensas evaluado resulto como:\n",
    "\n",
    "$$\n",
    "r_{\\pi_1} = \n",
    "\\begin{bmatrix}\n",
    "1 \\\\ \n",
    "1 \\\\ \n",
    "1 \\\\ \n",
    "0 \\\\ \n",
    "0 \\\\ \n",
    "0 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Con esta información ya se puede encontrar el valor de la política lo cual se realiza en la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7109d3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.28247549]\n",
      " [2.28247549]\n",
      " [1.63909314]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Matriz de transición\n",
    "P = np.array([\n",
    "    [0, 0.3, 0.7, 0, 0, 0], \n",
    "    [0, 0.3, 0.7, 0, 0, 0], \n",
    "    [0.4, 0, 0, 0.6, 0, 0], \n",
    "    [0, 0, 0, 0, 0.6, 0.4], \n",
    "    [0, 0, 0, 0, 0, 0], \n",
    "    [0, 0, 0, 0, 0, 0]\n",
    "]) \n",
    "# Vector de recompensas.\n",
    "r = np.array([[1], [1], [1], [0], [0], [0]])\n",
    "\n",
    "g = 0.7 # Factor de descuento establecido para el problema\n",
    "M = np.eye(6) - g*P\n",
    "M_inv = np.linalg.inv(M)\n",
    "v1 = np.dot(M_inv, r)\n",
    "\n",
    "print(v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1359a3d3",
   "metadata": {},
   "source": [
    "**Valor de la política $\\pi_2$**\n",
    "\n",
    "También podemos evaluar el valor de la politica $\\pi_2$, la que realiza la acción $a_2$ a cada estado del problema.\n",
    "\n",
    "Recordemos que la matriz de transición es:\n",
    "\n",
    "$$\n",
    "P_{\\pi_2} = \n",
    "\\begin{bmatrix}\n",
    "0.3 & 0 & 0 & 0.7 & 0 & 0\\\\ \n",
    "0.3 & 0 & 0 & 0.7 & 0 & 0\\\\ \n",
    "0 & 0.4 & 0.6 & 0 & 0 & 0\\\\ \n",
    "0 & 0 & 0 & 0 & 0 & 0\\\\ \n",
    "0 & 0 & 0 & 0 & 0 & 0\\\\ \n",
    "0 & 0 & 0 & 0 & 0 & 0\\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Y el vector de recompensas es:\n",
    "\n",
    "$$\n",
    "r_{\\pi_2} = \n",
    "\\begin{bmatrix}\n",
    "1 \\\\ \n",
    "1 \\\\ \n",
    "1 \\\\ \n",
    "0 \\\\ \n",
    "0 \\\\ \n",
    "0 \\\\ \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Con esta información ya se puede encontrar el valor de la política lo cual se realiza en la siguiente celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb7f74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.26582278]\n",
      " [1.26582278]\n",
      " [2.33522479]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Matriz de transición\n",
    "P = np.array([\n",
    "    [0.3, 0, 0, 0.7, 0, 0],\n",
    "    [0.3, 0, 0, 0.7, 0, 0],\n",
    "    [0, 0.4, 0.6, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0]\n",
    "])\n",
    "# Vector de recompensas.\n",
    "r = np.array([[1], [1], [1], [0], [0], [0]])\n",
    "\n",
    "g = 0.7 # Factor de descuento establecido para el problema\n",
    "M = np.eye(6) - g*P\n",
    "M_inv = np.linalg.inv(M)\n",
    "v2 = np.dot(M_inv, r)\n",
    "\n",
    "print(v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9878e5e",
   "metadata": {},
   "source": [
    "## Iteración sobre politicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444d452b",
   "metadata": {},
   "source": [
    "Esta metodología permite encontrar una política óptima a través de los siguientes pasos:\n",
    "\n",
    "1. Iniciar con una política $\\pi$, la cual puede ser aleatoria\n",
    "2. Iterativamente:\n",
    "    \n",
    "    2.1 Evaluar la política:\n",
    "    \n",
    "    $$ v_\\pi = (I - \\gamma P_\\pi)^{-1} r_\\pi $$\n",
    "    \n",
    "    2.2 Mejorar la política\n",
    "    \n",
    "    $$ \\pi'(s) = \\arg \\max_{a} R_{s,a}  + \\gamma P_{s, a} v_\\pi $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3e6e6b",
   "metadata": {},
   "source": [
    "Para el problema se iniciara con la política $\\pi_1$, la que asigna la acción $a_1$ a cada estado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f7910",
   "metadata": {},
   "source": [
    "## Iteración 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf8560",
   "metadata": {},
   "source": [
    "El valor de la política ya habia sido evaluado, el cual dio como resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43573e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.28247549]\n",
      " [2.28247549]\n",
      " [1.63909314]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Matriz de transición\n",
    "P = np.array([\n",
    "    [0, 0.3, 0.7, 0, 0, 0], \n",
    "    [0, 0.3, 0.7, 0, 0, 0], \n",
    "    [0.4, 0, 0, 0.6, 0, 0], \n",
    "    [0, 0, 0, 0, 0.6, 0.4], \n",
    "    [0, 0, 0, 0, 0, 0], \n",
    "    [0, 0, 0, 0, 0, 0]\n",
    "]) \n",
    "# Vector de recompensas.\n",
    "r = np.array([[1], [1], [1], [0], [0], [0]])\n",
    "\n",
    "g = 0.7 \n",
    "M = np.eye(6) - g*P\n",
    "M_inv = np.linalg.inv(M)\n",
    "v = np.dot(M_inv, r)\n",
    "\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe6c502",
   "metadata": {},
   "source": [
    "### $S_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1963fa80",
   "metadata": {},
   "source": [
    "#### $a_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e767d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.28247549])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + g*np.dot([0, 0.3, 0.7, 0, 0, 0], v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b7fc43",
   "metadata": {},
   "source": [
    "#### $a_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5e122bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.47931985])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + g*np.dot([0.3, 0, 0, 0.7, 0, 0], v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33821894",
   "metadata": {},
   "source": [
    "En este estado la acción que maximiza el valor es $a_1$, por lo tanto la elegimos y como se tenia esa acción no se actualiza la política."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74157f36",
   "metadata": {},
   "source": [
    "### $S_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec680adf",
   "metadata": {},
   "source": [
    "#### $a_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8db1df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.28247549])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + g*np.dot([0, 0.3, 0.7, 0, 0, 0], v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4d0aaa",
   "metadata": {},
   "source": [
    "#### $a_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4692421a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.47931985])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + g*np.dot([0.3, 0, 0, 0.7, 0, 0], v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a3550",
   "metadata": {},
   "source": [
    "En este estado la acción que maximiza el valor es $a_1$, por lo tanto la elegimos y como se tenia esa acción no se actualiza la política."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842a4e90",
   "metadata": {},
   "source": [
    "### $S_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c8d1b6",
   "metadata": {},
   "source": [
    "#### $a_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "310289f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.63909314])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + g*np.dot([0.4, 0, 0, 0.6, 0, 0], v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bc8bb",
   "metadata": {},
   "source": [
    "#### $a_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b09bffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.32751225])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + g*np.dot([0, 0.4, 0.6, 0, 0, 0], v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d80a6",
   "metadata": {},
   "source": [
    "En este estado la acción que maximiza el valor es $a_2$ por lo tanto la elegimos y la politica tendra que ser actualiza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91cff37",
   "metadata": {},
   "source": [
    "### $S_4$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4703f62c",
   "metadata": {},
   "source": [
    "#### $a_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b41a0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 + g*np.dot([0, 0, 0, 0, 0.6, 0.4], v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695ccc1",
   "metadata": {},
   "source": [
    "#### $a_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e47b7b",
   "metadata": {},
   "source": [
    "No hay otro con cual comparar entonces la acción $a_1$ se mantiene y la política no se actualiza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423e0322",
   "metadata": {},
   "source": [
    "### $S_5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4073f3ac",
   "metadata": {},
   "source": [
    "Para este estado no hay acciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a61895",
   "metadata": {},
   "source": [
    "### $S_6$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb6757",
   "metadata": {},
   "source": [
    "Para este estado no hay acciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe69deb",
   "metadata": {},
   "source": [
    "## Iteración 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198f21e3",
   "metadata": {},
   "source": [
    "Se actualiza la política en el estado $S_3$ con la acción $a_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d562be90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.33333333]\n",
      " [3.33333333]\n",
      " [3.33333333]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Matriz de transición\n",
    "P = np.array([\n",
    "    [0, 0.3, 0.7, 0, 0, 0], \n",
    "    [0, 0.3, 0.7, 0, 0, 0], \n",
    "    [0, 0.4, 0.6, 0, 0, 0], # Parte actualizada.\n",
    "    [0, 0, 0, 0, 0.6, 0.4], \n",
    "    [0, 0, 0, 0, 0, 0], \n",
    "    [0, 0, 0, 0, 0, 0]\n",
    "]) \n",
    "# Vector de recompensas.\n",
    "# La recompensa para el estado 3 ya tenia el valor de 1.\n",
    "r = np.array([[1], [1], [1], [0], [0], [0]])\n",
    "\n",
    "g = 0.7 \n",
    "M = np.eye(6) - g*P\n",
    "M_inv = np.linalg.inv(M)\n",
    "v = np.dot(M_inv, r)\n",
    "\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a8fb58",
   "metadata": {},
   "source": [
    "### $S_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23796b4b",
   "metadata": {},
   "source": [
    "#### $a_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26307079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.33333333])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + g*np.dot([0, 0.3, 0.7, 0, 0, 0], v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48079759",
   "metadata": {},
   "source": [
    "#### $a_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4daef166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.7])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + g*np.dot([0.3, 0, 0, 0.7, 0, 0], v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9ef746",
   "metadata": {},
   "source": [
    "Elegimos la acción $a_1$ y como se tenia esa acción no se actualiza la política."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9d7072",
   "metadata": {},
   "source": [
    "### $S_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0366e32",
   "metadata": {},
   "source": [
    "#### $a_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73b28376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.33333333])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + g*np.dot([0, 0.3, 0.7, 0, 0, 0], v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbbaa13",
   "metadata": {},
   "source": [
    "#### $a_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cf21c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.7])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + g*np.dot([0.3, 0, 0, 0.7, 0, 0], v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2bb463",
   "metadata": {},
   "source": [
    "Elegimos la acción $a_1$ y como se tenia esa acción no se actualiza la política."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc7d34",
   "metadata": {},
   "source": [
    "### $S_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877eb88a",
   "metadata": {},
   "source": [
    "#### $a_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95e72f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.93333333])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + g*np.dot([0.4, 0, 0, 0.6, 0, 0], v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb0f65f",
   "metadata": {},
   "source": [
    "#### $a_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60422630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.33333333])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + g*np.dot([0, 0.4, 0.6, 0, 0, 0], v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca540d",
   "metadata": {},
   "source": [
    "Elegimos la acción $a_2$ y como se tenia esa acción no se actualiza la política."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6986330c",
   "metadata": {},
   "source": [
    "### $S_4$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7c0598",
   "metadata": {},
   "source": [
    "#### $a_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d99c6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 + g*np.dot([0, 0, 0, 0, 0.6, 0.4], v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc13dd9",
   "metadata": {},
   "source": [
    "#### $a_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c416e9",
   "metadata": {},
   "source": [
    "No hay otro con cual comparar entonces la acción $a_1$ se mantiene y la política no se actualiza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a209a87",
   "metadata": {},
   "source": [
    "### $S_5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf640da",
   "metadata": {},
   "source": [
    "Para este estado no hay acciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbff727",
   "metadata": {},
   "source": [
    "### $S_6$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f454f42b",
   "metadata": {},
   "source": [
    "Para este estado no hay acciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd959e",
   "metadata": {},
   "source": [
    "Como ya no hay ninguna acción que actualizar en la politica, el algoritmo ha convergido, entonces hemos llegado a la política optima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cce934",
   "metadata": {},
   "source": [
    "## Política optima - resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16ea75ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.33333333]\n",
      " [3.33333333]\n",
      " [3.33333333]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Matriz de transición\n",
    "P = np.array([\n",
    "    [0, 0.3, 0.7, 0, 0, 0], \n",
    "    [0, 0.3, 0.7, 0, 0, 0], \n",
    "    [0, 0.4, 0.6, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0.6, 0.4], \n",
    "    [0, 0, 0, 0, 0, 0], \n",
    "    [0, 0, 0, 0, 0, 0]\n",
    "]) \n",
    "# Vector de recompensas.\n",
    "r = np.array([[1], [1], [1], [0], [0], [0]])\n",
    "\n",
    "g = 0.7 \n",
    "M = np.eye(6) - g*P\n",
    "M_inv = np.linalg.inv(M)\n",
    "v = np.dot(M_inv, r)\n",
    "\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7053b7e4",
   "metadata": {},
   "source": [
    "# Forma automatizada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33958e00",
   "metadata": {},
   "source": [
    "Se establecen la matriz de transición y vector de recompensas para cada politica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d98cc602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de transición y vector de recompensas de la politica 1\n",
    "\n",
    "P1 = np.array([\n",
    "    [0, 0.3, 0.7, 0, 0, 0], \n",
    "    [0, 0.3, 0.7, 0, 0, 0], \n",
    "    [0.4, 0, 0, 0.6, 0, 0], \n",
    "    [0, 0, 0, 0, 0.6, 0.4], \n",
    "    [0, 0, 0, 0, 0, 0], \n",
    "    [0, 0, 0, 0, 0, 0]\n",
    "]) \n",
    "r1 = np.array([[1], [1], [1], [0], [0], [0]])\n",
    "\n",
    "# Matriz de transición y vector de recompensas de la politica 2\n",
    "\n",
    "P2 = np.array([\n",
    "    [0.3, 0, 0, 0.7, 0, 0],\n",
    "    [0.3, 0, 0, 0.7, 0, 0],\n",
    "    [0, 0.4, 0.6, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0]\n",
    "])\n",
    "r2 = np.array([[1], [1], [1], [0], [0], [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985547fb",
   "metadata": {},
   "source": [
    "Al igual que en el proceso manual para el problema se iniciara con la política  $\\pi_1$ , la que asigna la acción  $a_1$  a cada estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b4fbecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de transición\n",
    "P = np.array([\n",
    "    [0, 0.3, 0.7, 0, 0, 0], \n",
    "    [0, 0.3, 0.7, 0, 0, 0], \n",
    "    [0.4, 0, 0, 0.6, 0, 0], \n",
    "    [0, 0, 0, 0, 0.6, 0.4], \n",
    "    [0, 0, 0, 0, 0, 0], \n",
    "    [0, 0, 0, 0, 0, 0]\n",
    "]) \n",
    "# Vector de recompensas.\n",
    "r = np.array([[1], [1], [1], [0], [0], [0]])\n",
    "\n",
    "g = 0.7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4287cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 representa escoger la politica 1\n",
    "# 1 representa escoger la politica 2\n",
    "# Como se determino empezar con la politica 1 por eso el vector esta lleno de ceros.\n",
    "mov = [0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afc8550c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERACION  1\n",
      "\n",
      "Valor de la politica \n",
      " [[2.28247549]\n",
      " [2.28247549]\n",
      " [1.63909314]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]] \n",
      "\n",
      "Estado 1\n",
      "[2.28247549]\n",
      "[1.47931985]\n",
      "\n",
      "Estado 2\n",
      "[2.28247549]\n",
      "[1.47931985]\n",
      "\n",
      "Estado 3\n",
      "[1.63909314]\n",
      "[2.32751225]\n",
      "Cambio en la politica\n",
      "\n",
      "Estado 4\n",
      "[0.]\n",
      "[0.]\n",
      "\n",
      "Estado 5\n",
      "[0.]\n",
      "[0.]\n",
      "\n",
      "Estado 6\n",
      "[0.]\n",
      "[0.]\n",
      "\n",
      "----------------------------------------------\n",
      "ITERACION  2\n",
      "\n",
      "Valor de la politica \n",
      " [[3.33333333]\n",
      " [3.33333333]\n",
      " [3.33333333]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]] \n",
      "\n",
      "Estado 1\n",
      "[3.33333333]\n",
      "[1.7]\n",
      "\n",
      "Estado 2\n",
      "[3.33333333]\n",
      "[1.7]\n",
      "\n",
      "Estado 3\n",
      "[1.93333333]\n",
      "[3.33333333]\n",
      "\n",
      "Estado 4\n",
      "[0.]\n",
      "[0.]\n",
      "\n",
      "Estado 5\n",
      "[0.]\n",
      "[0.]\n",
      "\n",
      "Estado 6\n",
      "[0.]\n",
      "[0.]\n",
      "\n",
      "----------------------------------------------\n",
      "Ya no se realizaron mas cambios en el valor de la politica\n",
      "Por lo tanto la politica optima es: \n",
      "\n",
      "[[3.33333333]\n",
      " [3.33333333]\n",
      " [3.33333333]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "n_states = 6\n",
    "change_P_v = True\n",
    "it = 1\n",
    "\n",
    "while change_P_v:\n",
    "    M = np.eye(6) - g*P\n",
    "    M_inv = np.linalg.inv(M)\n",
    "    v = np.dot(M_inv, r)\n",
    "    \n",
    "    \n",
    "\n",
    "    print('ITERACION ', it)\n",
    "#     print('\\nMatriz de transición\\n', P)\n",
    "#     print('\\nVector de recompensas\\n', r)\n",
    "    print('\\nValor de la politica \\n', v, '\\n')\n",
    "    \n",
    "    change_P_v = False\n",
    "    \n",
    "    for state in range(n_states):\n",
    "        print('Estado', state+1)\n",
    "        x1 = r1[state, 0] + g*np.dot(P1[state], v)\n",
    "        x2 = r2[state, 0] + g*np.dot(P2[state], v)\n",
    "\n",
    "        print(x1)\n",
    "        print(x2)\n",
    "        max_mov = np.argmax([x1[0], x2[0]])\n",
    "\n",
    "        if mov[state] != max_mov:\n",
    "            print(\"Cambio en la politica\")\n",
    "            change_P_v = True\n",
    "            # Si la politica que ya tenia era la 1 se cambia a la 2\n",
    "            if mov[state] == 0:\n",
    "                P[state] = P2[state]\n",
    "                r[state] = r2[state]\n",
    "                mov[state] = 1\n",
    "            # Si la politica que ya tenia era la 2 se cambia a la 1\n",
    "            else:\n",
    "                P[state] = P1[state]\n",
    "                r[state] = r1[state]\n",
    "                mov[state] = 0\n",
    "        print()\n",
    "        \n",
    "    it += 1\n",
    "    print('----------------------------------------------')\n",
    "\n",
    "print('Ya no se realizaron mas cambios en el valor de la politica')    \n",
    "print('Por lo tanto la politica optima es: \\n')\n",
    "print(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
